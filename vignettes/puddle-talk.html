<!DOCTYPE html>
<html>
<head>
<title>Talk at Puddle • Computational and statistical learning theory for language.</title>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/><link href="puddle-talk.css" rel="stylesheet" type="text/css"/></head>
<body>
<header>
<h2><a href="../index.html">La Vignetterie<span class="nowidth">.</span></a></h2>
</header>
<main>
<h1>Talk at Puddle • Computational and statistical learning theory for language.</h1>
<!-- l. 7 --><p class="noindent">I am speaking at Puddle on Friday of seventh week (28 November) at 3 pm in
Lecture Theatre B of computer science. (Enter via the <a href="/maps.app.goo.gl/kCmmU5KUupJZPgv87">door</a> to Lecture Theatres A
and B, or go to the <a href="https://maps.app.goo.gl/CqycjniGSzpSQiJJA">reception<span class="nowidth">.</span></a>)
</p><!-- l. 9 --><p class="indent">   Puddle is a seminar arranged by doctorants in computer science. As far as I can
tell, it nearly exclusively hosts (1) category theorists and (2) token philosophers.
(Plausibly this talk has more to do with computer science than, er, modal
potentialism.)
</p><!-- l. 11 --><p class="indent">   Consider the following roughly Chomskyan argument. (1) Without innate
knowledge of language, first language acquisition would be so hard as to be
impracticable. (2) But we generally succeed in acquiring our first language. (3) So we
must have innate knowledge of language. The argument can only be assessed with
respect to some understanding of hardness. Some linguists, cognitive scientists, and
philosophers appeal to learning theory: hardness is (sample or computational)
complexity. I discuss two difficulties for this approach. First, the applicability of these
learning-theoretic results to cases of language acquisition is unclear. Unlearnability
results are régime-dependent, and it’s not obvious that régimes where unlearnability
results hold are any more plausible than other régimes. Second, many learning
theory-inspired accounts of what innate knowledge actually is do not seem to
explain learnability. I propose an explanation. The problem is the assumption
that language learning should scale asymptotically, rather than simply on
plausible inputs. I suggest this assumption arises from a confusion between
two different forms of computationalism about cognition; one justifies the
assumption, but only the other is well-motivated. If I have time, I will explain how
similar considerations apply to arguments against linguistic competence in
language models appealing to learning or complexity theory. I shall assume no
substantial prior knowledge of philosophy of language, Chomsky, computational
complexity, or philosophy of mind. For an informal exposition of some of these
ideas, see a short <a href="https://vignetterie.org/vignettes/complexity-philosophy.html">note</a> on computational complexity and philosophy; and a
<a href="https://vignetterie.org/vignettes/thesis-proposal.html">sketch</a> of the thesis from work on which I shall draw the material in the
talk.
</p><h2>À propos.</h2>
<h3>Étiquettes.</h3>
<p><a href="../etiquettes/philo.html">philo<span class="nowidth">,</span></a> <a href="../etiquettes/computational-complexity.html">computational complexity<span class="nowidth">,</span></a> <a href="../etiquettes/learning-theory.html">learning theory<span class="nowidth">,</span></a> <a href="../etiquettes/chomsky.html">Chomsky<span class="nowidth">,</span></a> <a href="../etiquettes/linguistics.html">linguistics<span class="nowidth">,</span></a> <a href="../etiquettes/talks.html">talks<span class="nowidth">.</span></a></p>
<h3>Mises à jour.</h3>
<ul class="plainlist">
<li>J.P. Loo (24 novembre 2025): New Puddle abstract.</li>
<li>J.P. Loo (14 novembre 2025): Puddle.</li>
</ul>
</main>
</body>
</html>